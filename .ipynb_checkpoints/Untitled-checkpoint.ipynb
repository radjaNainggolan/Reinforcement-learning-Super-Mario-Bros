{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf90d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, time, datetime, os, copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "if gym.__version__ < '0.26':\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
    "else:\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='human', apply_api_compatibility=True)\n",
    "\n",
    "env = JoypadSpace(env,SIMPLE_MOVEMENT)\n",
    "\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"return every skip-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self.skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        for everu skiped frame and action taken in that frame\n",
    "        add reward on acumulated reward\n",
    "        and in the end return the state, acumulated reward, done, trunc and info\n",
    "        for the state where agent came after all skiped frames\n",
    "        \"\"\"\n",
    "        total_reward = 0.0\n",
    "\n",
    "        for i in range(self.skip):\n",
    "            obs, reward, done, trunc, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunc, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        wrapper that will transform\n",
    "        rgb frames into grayscale frames\n",
    "        this custom wrapper inherits the Observation wrapper from gym\n",
    "        and as argument it takse the enviornment\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2] #from enviornment obseravtion shape we take just size of frame and not the channel size\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self,observation):\n",
    "        \"\"\"permute [H,W,C] to [C,H,W]\"\"\"\n",
    "        observation = np.transpose(observation, (2, 0, 1)) #here we change the positions of channel, height and width\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)# create tenosr with these infos\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)# call for changing positions of c, h, w\n",
    "        transform = T.Grayscale() #instace of GrayScale transform function\n",
    "        observation = transform(observation) #GrayScale Trasnforamtion\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        \"\"\"\n",
    "        custom Resize wrapper that will resize frame to new size\n",
    "        it also inherits Obseravtion wrapper from gym\n",
    "        and takes env and shaep(int) for arguments\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape) #if shape is instance of int crate tuple (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape) #if it is not instace its array so create tuple from array\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:] #new observation shape will be self.shape which\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transform = T.Compose([T.Resize(self.shape), T.Normalize(0, 255)])\n",
    "        observation = transform(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "#print(env.observation_space.shape)\n",
    "\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "\n",
    "if gym.__version__ < '0.26':\n",
    "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
    "else:\n",
    "    env = FrameStack(env, num_stack=4)\n",
    "\n",
    "#print(env.observation_space.shape)\n",
    "\n",
    "\n",
    "\n",
    "class Mario:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        \"\"\" agent class\"\"\"\n",
    "\n",
    "        self.state_dim = state_dim #(4, 84, 84) the size of frame\n",
    "        self.action_dim = action_dim #number of action that agent can take\n",
    "        self.save_dir = save_dir #directorium where \n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\" #using gpu for computation\n",
    "\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float() #initializing agent's DQN\n",
    "        self.net = self.net.to(device=self.device) #to gpu\n",
    "\n",
    "        self.exploration_rate = 1 # exploratio rate , if bigger the bigger chance for exploring\n",
    "        self.exploration_rate_decay = 0.99999975 # value that will decrease exploration rate\n",
    "        self.exploration_rate_min = 0.1 # the minimun exploration rate\n",
    "        self.curr_step = 0 #varibel that will save the number of steps that are taken\n",
    "\n",
    "        self.save_every = 5e5 #enveiroment situtation will be saved at every 500000-th step\n",
    "\n",
    "        self.memory = deque(maxlen=100000)  #list where agents experience will be saved after every action\n",
    "        #experience = next_state, state, action, reward, done\n",
    "        self.batch_size = 32 #number of samples that will be pulled out of memory and used for model updating\n",
    "\n",
    "        self.gamma = 0.9 #??????????????????????????\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025) #optimization algorithm that will be used to optimize loss function\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss() #function that will be used to calculate deviation between expected and calculated result of aproximation on q funciton\n",
    "\n",
    "        self.burnin = 1e4 #minimum of experience before learning\n",
    "        self.learn_every = 3 #number of experience between updates of Q network\n",
    "        self.sync_every = 1e4 #number of experience between syncornisation of Q target network\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        if np.random.rand() < self.exploration_rate: #if exploration rate is bigger than random number from 0 to 1 than do random action\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__() #transform to array\n",
    "            state = torch.tensor(state, device=self.device).unsqueeze(0) #create tensor from state data\n",
    "            action_values = self.net(state, model=\"online\") #se\n",
    "            action_idx = torch.argmax(action_values, axis=1).item() #will get id of action that gives the biggest reward\n",
    "\n",
    "        self.exploration_rate *= self.exploration_rate_decay #make exploratio rate smaller by multiplying it with decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate) #exploratio rate can not be less than its minimum\n",
    "\n",
    "        self.curr_step += 1\n",
    "        return action_idx\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        transform data from tuples to array, make tensors with that data and push them into memeory as tuple\n",
    "        \"\"\"\n",
    "        def first_if_tuple(x):\n",
    "            return x[0] if isinstance(x, tuple) else x\n",
    "        state = first_if_tuple(state).__array__()\n",
    "        next_state = first_if_tuple(next_state).__array__()\n",
    "\n",
    "        state = torch.tensor(state, device=self.device)\n",
    "        next_state = torch.tensor(next_state, device=self.device)\n",
    "        action = torch.tensor([action], device=self.device)\n",
    "        reward = torch.tensor([reward], device=self.device)\n",
    "        done = torch.tensor([done], device=self.device)\n",
    "\n",
    "        self.memory.append((state, next_state, action, reward, done))\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        pull random 32 samples from memory\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))# first it will\n",
    "        #unpack batch which means that every tensor inside tuple will be separet agument inside zip function,\n",
    "        #than it will concatenate all the tensors along the new dimension\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze() #returns the experience\n",
    "        #squeeze will remove all the dimension of size 1\n",
    "\n",
    "    def td_esitmate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[np.arange(0, self.batch_size), action]\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[np.arange(0, self.batch_size), best_action]\n",
    "\n",
    "        return (reward + (1-done.float()) * self.gamma * next_Q).float()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_taget(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
    "\n",
    "    def save(self):\n",
    "        save_path = (self.save_dir/f\"mario_net{int(self.curr_step // self.save_every)}.chkpt\")\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_taget()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        td_est = self.td_esitmate(state, action)\n",
    "\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)\n",
    "\n",
    "\n",
    "\n",
    "class MarioNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\"\n",
    "        class tha inherits nn.Module which is base class for all neural network modules\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {h}\")\n",
    "\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )\n",
    "\n",
    "        self.target = copy.deepcopy(self.online)\n",
    "\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
    "            plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 40000\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)\n",
    "\n",
    "# state = env.reset()\n",
    "# next_state, reward, done, trunc, info = env.step(env.action_space.sample())\n",
    "# print(len(next_state.__array__()))\n",
    "# #print(\"\\n\")\n",
    "# #print(next_state.__array__()[0])\n",
    "#\n",
    "# print(f\"{next_state.__array__()} state, \\n {reward} reward, \\n {done} done, \\n {info} info\")\n",
    "\n",
    "# episodes = 2\n",
    "# for e in range(episodes):\n",
    "#     state = env.reset()\n",
    "#     while True:\n",
    "#         env.render()\n",
    "#         state, reward, done, trunc, info = env.step(env.action_space.sample())\n",
    "#\n",
    "#         if done:\n",
    "#             break\n",
    "#\n",
    "# env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
